{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## CS4287 - Neural Network\n",
    "### Assignment 1 - 4th Year Semester 1 2024\n",
    "\n",
    "Adam Collins: 21332967\n",
    "\n",
    "Italo da Silva: 21326312"
   ],
   "id": "5e61c443c72ca5a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The Code executes to the end without an error. ",
   "id": "83d747b7aa37043c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score"
   ],
   "id": "e2639accb1bc1569",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. The Data Set \n",
    "#   (a). Visualisation of key attributes.\n",
    "\n",
    "# Loading the csv file and viewing first rows using pandas library\n",
    "housing_data = pd.read_csv('Melbourne_housing_FULL.csv', low_memory=False) \n",
    "\n",
    "# Filtering the Dataset\n",
    "# Dropping every cell that contains a NaN value (there was alot)\n",
    "columns = [\"Suburb\",\"Rooms\",\"Type\",\"Price\",\"Method\",\"SellerG\",\"Date\",\"Distance\",\"Bedroom2\",\"Bathroom\",\"Car\",\"Landsize\",\"BuildingArea\",\"YearBuilt\",\"CouncilArea\",\"Lattitude\",\"Longtitude\",\"Regionname\",\"Propertycount\"]\n",
    "housing_data = housing_data.dropna(subset=columns)\n",
    "\n",
    "# Dropping all the columns that don't contain numbers (there was alot too )\n",
    "housing_data.drop([\"Suburb\", \"Address\", \"Type\", \"Method\", \"SellerG\", \"CouncilArea\", \"Regionname\", \"Date\"], inplace=True, axis=\"columns\")\n",
    "housing_data.head()"
   ],
   "id": "85c8463c5412e41a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Draw graph of correlation between Price and Building Area\n",
    "\n",
    "## Converting the data in the columns to numeric \n",
    "housing_data[['Price', 'BuildingArea']] = housing_data[['Price', 'BuildingArea']].apply(pd.to_numeric)\n",
    "\n",
    "## Reshaping the data to a 2D array in order to plot it in the Linear Regression \n",
    "price = housing_data['Price'].values.reshape(-1,1)\n",
    "building_area = housing_data['BuildingArea'].values.reshape(-1,1)\n",
    "\n",
    "## Creating a Linear Regression model to predict the data for Building Area based on the Price\n",
    "reg = LinearRegression().fit(price, building_area)\n",
    "\n",
    "## Functions to plot the graph\n",
    "plt.plot(housing_data[['Price']], housing_data[['BuildingArea']], '*')\n",
    "plt.plot(housing_data[['Price']], reg.predict(price), 'r')\n",
    "\n",
    "## Adding labels to the graph\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Building Area')\n",
    "plt.title('Price vs Building Area')\n",
    "plt.show()"
   ],
   "id": "97f09daa0f6dc489",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# From sample Assignment 1 provided by the lecturer.\n",
    "\n",
    "# Dataframe of key attributes\n",
    "housing_data.corr()\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = housing_data.corr()\n",
    "\n",
    "# Generate Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='jet', cbar=True, fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ],
   "id": "3e0e136f35147f78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # 1. The Data Set \n",
    "#   (b). Pre-Processing - Normalisation\n",
    "\n",
    "# Normalizing the data\n",
    "scaler = MinMaxScaler()\n",
    "housing_data[['Rooms', 'Price', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Propertycount']] = scaler.fit_transform(housing_data[['Rooms', 'Price', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Propertycount']])\n",
    "\n",
    "# Dropping Postcode, Landsize and Propertycount as the values are very close to 0\n",
    "# Also dropping Longitude and Langitude columns as they're useless for correlating price as we already have a Distance (to CBD) column.\n",
    "\n",
    "housing_data.drop([\"Postcode\", \"Landsize\", \"Propertycount\", \"Longtitude\", \"Lattitude\"], inplace=True, axis=\"columns\")\n",
    "\n",
    "# Sine we dropped alot of rows previously due to NaN values and Unnecessary data, the rows index stayed the same.\n",
    "# So we now need to reset the index back to normal to avoid confusion and keep them in order.\n",
    "housing_data = housing_data.reset_index(drop=True)\n",
    "\n",
    "housing_data.head()"
   ],
   "id": "fbf5fec53a14808",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rows = len(housing_data.axes[0])\n",
    "columns = len(housing_data.axes[1])\n",
    "\n",
    "print(\"Number of columns: \", columns)\n",
    "print(\"Number of rows: \", rows)"
   ],
   "id": "11082663ed5198e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select the features and target variable\n",
    "# Correlating the price of each property based on it's features\n",
    "features = ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'BuildingArea','YearBuilt']\n",
    "\n",
    "x = housing_data[features]\n",
    "y = housing_data['Price']\n",
    "\n",
    "# Dividing the price column into 5 sections (labels) in order to classify correctly and can be used in the output layer of our model.\n",
    "\n",
    "# 0.00 to 0.15 (first section)\n",
    "# 0.15 to 0.38 (second section)\n",
    "# 0.38 to 0.62 (third section)\n",
    "# 0.62 to 0.85 (fourth section)\n",
    "# 0.85 to 1.00 (last section)\n",
    "\n",
    "y = pd.cut(y, bins=[0.00, 0.15, 0.38, 0.62, 0.85, 1.00], labels=[0, 1, 2, 3, 4])\n",
    "print(y)\n",
    "\n",
    "# In order to validate and evaluate the model, we are splitting the dataset into two parts, training and testing data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "\n",
    "print(\"\\nx_train shape: \", x_train.shape[0])\n",
    "print(\"x_test shape: \", x_test.shape[0])"
   ],
   "id": "8916f7e1f6a79ff8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# One hot representation of the samples (Lab3 - Exercise 2)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 5)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 5)\n",
    "\n",
    "# Defining model\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model.add(tf.keras.layers.Dense(24, input_shape=(len(features),), activation='relu'))\n",
    "\n",
    "# Softmax classification - Converts the output for each class to a probability value between 0-1, which is exponentially normalized among the classes. \n",
    "model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "# TODO: Add comment saying why use Adam\n",
    "optimiser = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Compile the model with categorical_crossentropy loss function, we are using this function\n",
    "# because it computes the loss between the labels (the price labels we have previously classified) and predictions\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=100, verbose=1, validation_data=(x_test, y_test))"
   ],
   "id": "cf836a9a60844813",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
